{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "连接Google Drive"
      ],
      "metadata": {
        "id": "XhuImoJ8Qljs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PkxwtDkGkje",
        "outputId": "c1eedf70-32d1-4cfc-df40-b69903581259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CompVis/stable-diffusion.git"
      ],
      "metadata": {
        "id": "lG_vpnNglZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqznT9rjIHrq",
        "outputId": "f5c174a8-7e00-4e9e-90ab-d686bedf2b39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/stable-diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ckpt下载完成后，将权重文件直接改名为model.ckpt，并在models/ldm下创建文件夹stable-diffusion-v1，放置model.ckpt"
      ],
      "metadata": {
        "id": "dOJibWMGlsWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTl8DhLEoxJW",
        "outputId": "4609b97f-64f0-412b-cdcb-d9a0ff0fa14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-20 03:46:02--  https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 54.86.30.45, 34.200.6.138, 3.226.175.122, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.86.30.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/bb/f8/bbf8615bc677e0da011740279e930a6405f70d016bfa386ce8cfbc850b64162c/86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea?response-content-disposition=attachment%3B%20filename%3D%22sd-v1-1.ckpt%22&Expires=1671767162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JiL2Y4L2JiZjg2MTViYzY3N2UwZGEwMTE3NDAyNzllOTMwYTY0MDVmNzBkMDE2YmZhMzg2Y2U4Y2ZiYzg1MGI2NDE2MmMvODZjZDFkM2NjYjA0NGQ3YmE4ZGI3NDNkNzE3YzliYWM2MDNjNDA0MzUwOGFkMjU3MTM4M2Y5NTQzOTBmM2NlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMnNkLXYxLTEuY2twdCUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY3MTc2NzE2Mn19fV19&Signature=EGqhsE1jem3FcXXJ0Lm69hyYSKye3RoWP3g15Y1TkTHFnFfmt~db08FQtPS60LWNOuiKmMUK4VgSvHktqAp9-JQ9WKdyZZeCBBoN9~WkB6QdiRvhPo4ycTvbjQXMXmdUfNXTMB3ZeWow1dv7vykwnhgd44bq7vs-83K2DbsCuEhgRuIK8I8jZw4rgOjUE6A--HlHYWzBn7fbpSHl4L-q2b9UCjK0soplUzEjEiEqlP4br7FApgh2P4r92-ilqJeBtLJVwV~ei1IngYRONDjwHGM-UrwiqpfT7C8aPQTZOowKPK5vPPlINP~QhAzrV-JZuXlNYywWNFvdJ6fN15V40g__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-12-20 03:46:02--  https://cdn-lfs.huggingface.co/repos/bb/f8/bbf8615bc677e0da011740279e930a6405f70d016bfa386ce8cfbc850b64162c/86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea?response-content-disposition=attachment%3B%20filename%3D%22sd-v1-1.ckpt%22&Expires=1671767162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JiL2Y4L2JiZjg2MTViYzY3N2UwZGEwMTE3NDAyNzllOTMwYTY0MDVmNzBkMDE2YmZhMzg2Y2U4Y2ZiYzg1MGI2NDE2MmMvODZjZDFkM2NjYjA0NGQ3YmE4ZGI3NDNkNzE3YzliYWM2MDNjNDA0MzUwOGFkMjU3MTM4M2Y5NTQzOTBmM2NlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMnNkLXYxLTEuY2twdCUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY3MTc2NzE2Mn19fV19&Signature=EGqhsE1jem3FcXXJ0Lm69hyYSKye3RoWP3g15Y1TkTHFnFfmt~db08FQtPS60LWNOuiKmMUK4VgSvHktqAp9-JQ9WKdyZZeCBBoN9~WkB6QdiRvhPo4ycTvbjQXMXmdUfNXTMB3ZeWow1dv7vykwnhgd44bq7vs-83K2DbsCuEhgRuIK8I8jZw4rgOjUE6A--HlHYWzBn7fbpSHl4L-q2b9UCjK0soplUzEjEiEqlP4br7FApgh2P4r92-ilqJeBtLJVwV~ei1IngYRONDjwHGM-UrwiqpfT7C8aPQTZOowKPK5vPPlINP~QhAzrV-JZuXlNYywWNFvdJ6fN15V40g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.219.2, 13.227.219.41, 13.227.219.4, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.219.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265380512 (4.0G) [application/zip]\n",
            "Saving to: ‘sd-v1-1.ckpt’\n",
            "\n",
            "sd-v1-1.ckpt        100%[===================>]   3.97G  34.1MB/s    in 1m 49s  \n",
            "\n",
            "2022-12-20 03:47:52 (37.3 MB/s) - ‘sd-v1-1.ckpt’ saved [4265380512/4265380512]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "添加依赖库，requirements.txt文件库来自于environment.yaml中pip后的库，将==的版本号去掉"
      ],
      "metadata": {
        "id": "oNIHAB_fQuZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "64YZIdHrV4l_",
        "outputId": "f5ea3a95-c97e-4b2e-ad5b-0d8c21c816d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.3\n",
            "  Downloading albumentations-0.4.3.tar.gz (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 37.5 MB/s \n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.11.1-py3-none-any.whl (524 kB)\n",
            "\u001b[K     |████████████████████████████████| 524 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.6.0.66)\n",
            "Collecting pudb\n",
            "  Downloading pudb-2022.1.3.tar.gz (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting invisible-watermark\n",
            "  Downloading invisible_watermark-0.1.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2.9.0)\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.5.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting test-tube>=0.7.5\n",
            "  Downloading test_tube-0.7.5.tar.gz (21 kB)\n",
            "Collecting streamlit>=0.73.1\n",
            "  Downloading streamlit-1.16.0-py2.py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 61.7 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 565 kB/s \n",
            "\u001b[?25hCollecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 82.5 MB/s \n",
            "\u001b[?25hCollecting kornia==0.6\n",
            "  Downloading kornia-0.6.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (1.7.3)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 77.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from kornia==0.6->-r requirements.txt (line 16)) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from kornia==0.6->-r requirements.txt (line 16)) (1.13.0+cu116)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.8/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 10)) (1.3.5)\n",
            "Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 10)) (2.9.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio->-r requirements.txt (line 6)) (7.1.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.2.0-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (6.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (5.1.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (1.5.1)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (4.4.0)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (4.2.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 78.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (3.19.6)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (9.0.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (2.8.2)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 69.9 MB/s \n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (7.1.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.4 in /usr/local/lib/python3.8/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 11)) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (2.11.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit>=0.73.1->-r requirements.txt (line 11)) (3.11.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (5.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (0.19.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->kornia==0.6->-r requirements.txt (line 16)) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.20.3->test-tube>=0.7.5->-r requirements.txt (line 10)) (2022.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit>=0.73.1->-r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit>=0.73.1->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit>=0.73.1->-r requirements.txt (line 11)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit>=0.73.1->-r requirements.txt (line 11)) (1.24.3)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit>=0.73.1->-r requirements.txt (line 11)) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (2.8.8)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (2022.10.10)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (2.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (1.51.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 10)) (3.2.2)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit>=0.73.1->-r requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from diffusers->-r requirements.txt (line 2)) (3.8.2)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 82.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from diffusers->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers->-r requirements.txt (line 2)) (4.64.1)\n",
            "Collecting urwid>=1.1.1\n",
            "  Downloading urwid-2.1.2.tar.gz (634 kB)\n",
            "\u001b[K     |████████████████████████████████| 634 kB 77.3 MB/s \n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.6.0\n",
            "  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.5 MB/s \n",
            "\u001b[?25hCollecting jedi<1,>=0.18\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 70.3 MB/s \n",
            "\u001b[?25hCollecting urwid_readline\n",
            "  Downloading urwid_readline-0.13.tar.gz (7.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi<1,>=0.18->pudb->-r requirements.txt (line 4)) (0.8.3)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 54.0 MB/s \n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 53.6 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 81.8 MB/s \n",
            "\u001b[?25hCollecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->-r requirements.txt (line 8)) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (3.8.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 8)) (1.3.1)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from torch-fidelity->-r requirements.txt (line 13)) (0.14.0+cu116)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime->invisible-watermark->-r requirements.txt (line 5)) (1.7.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime->invisible-watermark->-r requirements.txt (line 5)) (1.12)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime->invisible-watermark->-r requirements.txt (line 5)) (1.2.1)\n",
            "Building wheels for collected packages: albumentations, test-tube, imgaug, validators, pudb, urwid, antlr4-python3-runtime, urwid-readline\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60778 sha256=a87ad877f0aca42226d447874f6040e1c6b6db728ff3801a32ea0e9c3e4e0683\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/37/4e/0bd417ba6a58f73329b825623d8c949e8e4ac2cdbd252b786d\n",
            "  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25357 sha256=697c2c50a9f10a339ebd3ce75588f8ba764668c4029c7723b76fd5667e64f877\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b0/3a/00ea66dbb0d9ce470ce1bdcb854a6fa030c279c316cb27ca9e\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654019 sha256=7fc8fd0a1201764e0bf85e919858f981e577f5afdf9bf79b7db2c40858a07376\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/23/e8/b1016c275f713978d312621da3c4f55920ec4297798aba8a5a\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=ddbc8766c1f7c3044b2cf5f8f0de71213303da11b24e6a6818ad40d233003751\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "  Building wheel for pudb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pudb: filename=pudb-2022.1.3-py3-none-any.whl size=70258 sha256=0f4e70e33e88e2da7d97f5cf99acdca81c42745e78de67156d6f9570529870c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/38/91/a0a7fa58d205af466e4e732c9d8ca00ee2cdb2e675a03811bc\n",
            "  Building wheel for urwid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for urwid: filename=urwid-2.1.2-cp38-cp38-linux_x86_64.whl size=257379 sha256=25890ace5c61494f4590e921400e1964923389a20181f31b2b13b34dac1f1bd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/71/e4/38b5d81438105d0e3db5016cf2eea6fa796d89d96a04451d4d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=a631c2322a18a39ac03cca7d5ab0fb71723d932f42c7c0d54b8ea88c3f685a67\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "  Building wheel for urwid-readline (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for urwid-readline: filename=urwid_readline-0.13-py3-none-any.whl size=7573 sha256=bbd7e72d0c379f7f575c44b1f86619c13ddaed7baf352b005f321678a03c37ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/10/cf/57c5430b5e5033995b239456813efa2587b486130a2688ab3d\n",
            "Successfully built albumentations test-tube imgaug validators pudb urwid antlr4-python3-runtime urwid-readline\n",
            "Installing collected packages: smmap, humanfriendly, urwid, pygments, gitdb, commonmark, coloredlogs, watchdog, validators, urwid-readline, torchmetrics, tokenizers, tensorboardX, semver, rich, pympler, pydeck, onnxruntime, onnx, lightning-utilities, jedi, imgaug, huggingface-hub, gitpython, blinker, antlr4-python3-runtime, transformers, torch-fidelity, test-tube, streamlit, pytorch-lightning, pudb, omegaconf, kornia, invisible-watermark, imageio-ffmpeg, einops, diffusers, albumentations\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.4.0\n",
            "    Uninstalling imgaug-0.4.0:\n",
            "      Successfully uninstalled imgaug-0.4.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-0.4.3 antlr4-python3-runtime-4.9.3 blinker-1.5 coloredlogs-15.0.1 commonmark-0.9.1 diffusers-0.11.1 einops-0.6.0 gitdb-4.0.10 gitpython-3.1.29 huggingface-hub-0.11.1 humanfriendly-10.0 imageio-ffmpeg-0.4.7 imgaug-0.2.6 invisible-watermark-0.1.5 jedi-0.18.2 kornia-0.6.0 lightning-utilities-0.4.2 omegaconf-2.3.0 onnx-1.12.0 onnxruntime-1.13.1 pudb-2022.1.3 pydeck-0.8.0 pygments-2.13.0 pympler-1.0.1 pytorch-lightning-1.8.5.post0 rich-12.6.0 semver-2.13.0 smmap-5.0.0 streamlit-1.16.0 tensorboardX-2.5.1 test-tube-0.7.5 tokenizers-0.13.2 torch-fidelity-0.3.0 torchmetrics-0.11.0 transformers-4.25.1 urwid-2.1.2 urwid-readline-0.13 validators-0.20.0 watchdog-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins",
                  "pygments"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
            "  Updating ./src/taming-transformers clone (to revision master)\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->taming-transformers) (4.4.0)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
            "  Updating ./src/clip clone (to revision main)\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip) (0.14.0+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2.10)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/stable-diffusion\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->latent-diffusion==0.0.1) (4.4.0)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文字转图片，--prompt后的引号内内容即为文字"
      ],
      "metadata": {
        "id": "dyLz5WVkQzqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/txt2img.py --prompt \"a photo of Pikachu\" --plms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJbh3wziWA0h",
        "outputId": "2e0bf8f5-f16f-49a4-a97e-d1803cc3b3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flag1\n",
            "Global seed set to 42\n",
            "flag2\n",
            "flag3\n",
            "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
            "Global Step: 194366\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.embeddings.position_ids', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "flag4\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/2 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (3, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:05<04:07,  5.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:05<02:01,  2.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:06<01:20,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:07<01:01,  1.33s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:08<00:50,  1.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:08<00:43,  1.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:09<00:39,  1.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:10<00:36,  1.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:11<00:33,  1.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:11<00:32,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:12<00:30,  1.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:13<00:29,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:14<00:28,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:14<00:27,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:15<00:26,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:16<00:25,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:17<00:25,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:17<00:24,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:18<00:23,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:19<00:22,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:20<00:22,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:20<00:21,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:21<00:20,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:22<00:19,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:23<00:19,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:23<00:18,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:24<00:17,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:25<00:16,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:26<00:16,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:27<00:15,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:27<00:14,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:28<00:13,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:29<00:13,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:30<00:12,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:30<00:11,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:31<00:10,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:32<00:10,  1.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:33<00:09,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:34<00:08,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:34<00:07,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:35<00:06,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:36<00:06,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:37<00:05,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:37<00:04,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:38<00:03,  1.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:39<00:03,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:40<00:02,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:41<00:01,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:41<00:00,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:42<00:00,  1.17it/s]\n",
            "\n",
            "data: 100% 1/1 [00:52<00:00, 52.61s/it]\n",
            "Sampling:  50% 1/2 [00:52<00:52, 52.61s/it]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (3, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:01<01:20,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:02<00:54,  1.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:03<00:45,  1.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:03<00:41,  1.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:04<00:38,  1.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:06<00:35,  1.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:07<00:34,  1.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:07<00:33,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:08<00:32,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:09<00:31,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:10<00:30,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:11<00:29,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:11<00:28,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:12<00:28,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:13<00:27,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:14<00:26,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:15<00:25,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:15<00:24,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:16<00:24,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:17<00:23,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:18<00:22,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:19<00:21,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:20<00:21,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:20<00:20,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:21<00:19,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:22<00:18,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:23<00:17,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:24<00:17,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:24<00:16,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:25<00:15,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:26<00:14,  1.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:27<00:13,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:28<00:12,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:28<00:12,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:29<00:11,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:30<00:10,  1.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:31<00:09,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:32<00:08,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:32<00:08,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:33<00:07,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:34<00:06,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:35<00:05,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:36<00:04,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:36<00:03,  1.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:37<00:03,  1.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:38<00:02,  1.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:39<00:01,  1.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:40<00:00,  1.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:40<00:00,  1.22it/s]\n",
            "\n",
            "data: 100% 1/1 [00:47<00:00, 47.32s/it]\n",
            "Sampling: 100% 2/2 [01:39<00:00, 49.96s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "图片转图片，--prompt后引号内容为文字，--init-img后为源图片路径"
      ],
      "metadata": {
        "id": "6KB67JmgQ-cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/img2img.py --prompt 'A painting of the traditional Chinese building.' --init-img 'OIP.jpeg'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKBtbYV2RYQS",
        "outputId": "7506724d-73b9-4e03-ef18-f2c9cf02b8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 42\n",
            "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
            "Global Step: 194366\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.\n",
            "  rank_zero_deprecation(\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "loaded input image of size (472, 266) from OIP.jpeg\n",
            "target t_enc is 37 steps\n",
            "Sampling:   0% 0/1 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[ARunning DDIM Sampling with 37 timesteps\n",
            "\n",
            "\n",
            "Decoding image:   0% 0/37 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:   3% 1/37 [00:00<00:11,  3.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:   5% 2/37 [00:00<00:08,  4.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:   8% 3/37 [00:00<00:07,  4.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  11% 4/37 [00:00<00:06,  5.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  14% 5/37 [00:01<00:06,  5.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  16% 6/37 [00:01<00:05,  5.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  19% 7/37 [00:01<00:05,  5.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  22% 8/37 [00:01<00:05,  5.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  24% 9/37 [00:01<00:05,  5.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  27% 10/37 [00:01<00:04,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  30% 11/37 [00:02<00:04,  5.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  32% 12/37 [00:02<00:04,  5.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  35% 13/37 [00:02<00:04,  5.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  38% 14/37 [00:02<00:04,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  41% 15/37 [00:02<00:03,  5.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  43% 16/37 [00:03<00:03,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  46% 17/37 [00:03<00:03,  5.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  49% 18/37 [00:03<00:03,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  51% 19/37 [00:03<00:03,  5.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  54% 20/37 [00:03<00:03,  5.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  57% 21/37 [00:03<00:02,  5.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  59% 22/37 [00:04<00:02,  5.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  62% 23/37 [00:04<00:02,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  65% 24/37 [00:04<00:02,  5.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  68% 25/37 [00:04<00:02,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  70% 26/37 [00:04<00:02,  5.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  73% 27/37 [00:05<00:01,  5.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  76% 28/37 [00:05<00:01,  5.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  78% 29/37 [00:05<00:01,  5.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  81% 30/37 [00:05<00:01,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  84% 31/37 [00:05<00:01,  5.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  86% 32/37 [00:05<00:00,  5.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  89% 33/37 [00:06<00:00,  5.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  92% 34/37 [00:06<00:00,  5.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  95% 35/37 [00:06<00:00,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image:  97% 36/37 [00:06<00:00,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Decoding image: 100% 37/37 [00:06<00:00,  5.38it/s]\n",
            "\n",
            "data: 100% 1/1 [00:07<00:00,  7.32s/it]\n",
            "Sampling: 100% 1/1 [00:07<00:00,  7.32s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/img2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "txt2img，无安全检查(⚠18+警告⚠)"
      ],
      "metadata": {
        "id": "UoPDN4jlsYLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/txt2img_nonfws.py --prompt \"a photo of nude beautiful woman\" --plms --n_samples 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6omaJuFDsbbD",
        "outputId": "21f1b6d4-eb15-4ac1-eeb1-58fcffba4e4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 42\n",
            "Loading model from models/ldm/stable-diffusion-v1/model.ckpt\n",
            "Global Step: 194366\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/2 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:04<03:22,  4.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:04<01:28,  1.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:04<00:52,  1.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:04<00:35,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:05<00:26,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:05<00:20,  2.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:05<00:17,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:05<00:14,  2.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:06<00:13,  3.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:06<00:12,  3.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:06<00:11,  3.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:06<00:10,  3.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:07<00:10,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:07<00:09,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:07<00:09,  3.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:07<00:08,  3.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:08<00:08,  3.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:08<00:08,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:08<00:07,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:08<00:07,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:09<00:07,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:09<00:07,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:09<00:06,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:09<00:06,  3.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:10<00:06,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:10<00:06,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:10<00:05,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:10<00:05,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:11<00:05,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:11<00:05,  3.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:11<00:04,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:12<00:04,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:12<00:04,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:12<00:04,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:12<00:03,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:13<00:03,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:13<00:03,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:13<00:03,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:13<00:02,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:14<00:02,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:14<00:02,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:14<00:02,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:14<00:01,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:15<00:01,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:15<00:01,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:15<00:01,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:15<00:00,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:16<00:00,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:16<00:00,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:16<00:00,  3.01it/s]\n",
            "\n",
            "data: 100% 1/1 [00:20<00:00, 20.48s/it]\n",
            "Sampling:  50% 1/2 [00:20<00:20, 20.48s/it]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:00<00:24,  1.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:00<00:17,  2.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:01<00:14,  3.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:01<00:13,  3.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:01<00:12,  3.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:01<00:11,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:02<00:11,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:02<00:11,  3.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:02<00:10,  3.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:02<00:10,  3.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:03<00:10,  3.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:03<00:09,  3.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:03<00:09,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:03<00:09,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:04<00:09,  3.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:04<00:08,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:04<00:08,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:04<00:08,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:05<00:07,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:05<00:07,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:05<00:07,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:05<00:07,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:06<00:06,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:06<00:06,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:06<00:06,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:06<00:06,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:07<00:05,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:07<00:05,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:07<00:05,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:07<00:05,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:08<00:04,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:08<00:04,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:08<00:04,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:08<00:04,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:09<00:03,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:09<00:03,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:09<00:03,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:09<00:03,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:10<00:02,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:10<00:02,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:10<00:02,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:11<00:02,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:11<00:01,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:11<00:01,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:11<00:01,  3.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:12<00:01,  3.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:12<00:00,  3.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:12<00:00,  3.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:12<00:00,  3.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:13<00:00,  3.82it/s]\n",
            "\n",
            "data: 100% 1/1 [00:13<00:00, 13.63s/it]\n",
            "Sampling: 100% 2/2 [00:34<00:00, 17.05s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/noNFWS/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    }
  ]
}